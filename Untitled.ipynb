{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Meals can be served</td>\n",
       "      <td>in rooms at 9:00 p. m.</td>\n",
       "      <td>'outside the room at 3:00 p. m.', 'in the dini...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>The local government can deal with the problem...</td>\n",
       "      <td>'If some tragedies occur again ', ' relevant d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The author called Tommy 's parents in order to</td>\n",
       "      <td>help them realize their influence on Tommy</td>\n",
       "      <td>'blame Tommy for his failing grades', 'blame T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>'idioms are the most important part in a langu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How can we deal with snake wounds according to...</td>\n",
       "      <td>Stay calm and do n't move .</td>\n",
       "      <td>'Cut the wound and suck the poison out .'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                Meals can be served   \n",
       "1           It can be inferred from the passage that   \n",
       "2     The author called Tommy 's parents in order to   \n",
       "3           It can be inferred from the passage that   \n",
       "4  How can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                             in rooms at 9:00 p. m.   \n",
       "1  The local government can deal with the problem...   \n",
       "2         help them realize their influence on Tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                        Stay calm and do n't move .   \n",
       "\n",
       "                                          distractor  \n",
       "0  'outside the room at 3:00 p. m.', 'in the dini...  \n",
       "1  'If some tragedies occur again ', ' relevant d...  \n",
       "2  'blame Tommy for his failing grades', 'blame T...  \n",
       "3  'idioms are the most important part in a langu...  \n",
       "4          'Cut the wound and suck the poison out .'  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example_words = ['python','pythoning','pythoner','pythoned','pythonly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for w in example_words:\n",
    "#     print(ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(lemmatizer.lemmatize(\"better\",pos='a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sw = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = set(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = dataset[['question']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# critical_question = []\n",
    "# for i in questions:\n",
    "#     s = ' '.join(i)\n",
    "#     sample = []\n",
    "#     for j in word_tokenize(s):\n",
    "#         if (j not in sw) and (j not in p) and (len(j)>2)and (j.isalpha()):\n",
    "#             sample.append(lemmatizer.lemmatize(j))\n",
    "#         else:\n",
    "#             pass\n",
    "#     critical_question.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = dataset[['answer_text']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "distractors = dataset[['distractor']].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentences = questions+answers+distractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Meals can be served'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(total_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meals can be served</td>\n",
       "      <td>in rooms at  p m</td>\n",
       "      <td>outside the room at  p m in the dining  room a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it can be inferred from the passage that</td>\n",
       "      <td>the local government can deal with the problem...</td>\n",
       "      <td>if some tragedies occur again   relevant depar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the author called tommy s parents in order to</td>\n",
       "      <td>help them realize their influence on tommy</td>\n",
       "      <td>blame tommy for his failing grades blame tommy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>idioms are the most important part in a langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how can we deal with snake wounds according to...</td>\n",
       "      <td>stay calm and do nt move</td>\n",
       "      <td>cut the wound and suck the poison out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                meals can be served   \n",
       "1           it can be inferred from the passage that   \n",
       "2      the author called tommy s parents in order to   \n",
       "3           it can be inferred from the passage that   \n",
       "4  how can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                                   in rooms at  p m   \n",
       "1  the local government can deal with the problem...   \n",
       "2         help them realize their influence on tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                          stay calm and do nt move    \n",
       "\n",
       "                                          distractor  \n",
       "0  outside the room at  p m in the dining  room a...  \n",
       "1  if some tragedies occur again   relevant depar...  \n",
       "2  blame tommy for his failing grades blame tommy...  \n",
       "3  idioms are the most important part in a langua...  \n",
       "4             cut the wound and suck the poison out   "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['question']]= pd.DataFrame(dataset.question.apply(round1))\n",
    "dataset[['answer_text']]= pd.DataFrame(dataset.answer_text.apply(round1))\n",
    "dataset[['distractor']]= pd.DataFrame(dataset.distractor.apply(round1))\n",
    "dataset[['question']]= pd.DataFrame(dataset.question.apply(round2))\n",
    "dataset[['answer_text']]= pd.DataFrame(dataset.answer_text.apply(round2))\n",
    "dataset[['distractor']]= pd.DataFrame(dataset.distractor.apply(round2))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset\n",
    "x = sample['question'].values.tolist()\n",
    "y = sample['answer_text'].values.tolist()\n",
    "z = sample['distractor'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = x+y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_all = []\n",
    "for z in total:\n",
    "    total_all.append(z.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(total_all, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('was', 0.6873770952224731),\n",
       " ('works', 0.5782934427261353),\n",
       " ('subtle', 0.5683582425117493),\n",
       " ('workmen', 0.5501658916473389),\n",
       " ('pedaling', 0.549849808216095),\n",
       " ('uesbeck', 0.5428876280784607)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('is',topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87253094"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similarity('sun','moon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(word1,word2):\n",
    "    return model.wv.similarity(word1,word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(df):\n",
    "    answer_text = df[['answer_text']].values.tolist()\n",
    "    l = 2\n",
    "    sen = answer_text[0]\n",
    "#     sen = word_tokenize(sen)\n",
    "    sent = []\n",
    "    for a in sen:\n",
    "        sent.append(round1(a))\n",
    "    \n",
    "    join_sent = str(' '.join(sent))\n",
    "    token = word_tokenize(join_sent)\n",
    "    dict_token = {}\n",
    "    for b in range(len(token)):\n",
    "        dict_token[b] = token[b]\n",
    "    disc= {}\n",
    "    for c in range(len(token)):\n",
    "        if (token[c] not in sw) and (token[c] not in p):\n",
    "            disc[c] = token[c]\n",
    "    \n",
    "#     print(disc)\n",
    "#     sm = []\n",
    "    dict_sm = {}\n",
    "    idx_dic = []\n",
    "#     sm = defaultdict(dict)\n",
    "    for d in range(len(token)):\n",
    "        if d not in (disc.keys()):\n",
    "            idx_dic.append(d)\n",
    "        else:\n",
    "            sim= dict(model.wv.most_similar(dict_token[d],topn=3))\n",
    "#             sm.append([e for e in sim.keys() if sim[e] >= 0.75])\n",
    "            dict_sm[d] = [e for e in sim.keys() if sim[e] >= 0.75]\n",
    "    \n",
    "    \n",
    "#     sm = defaultdict(dict)\n",
    "#     for f in (dict_token.keys()):\n",
    "#         if (f not in disc.keys()):\n",
    "#             pass\n",
    "#         else:\n",
    "#             for g in range(len(dict_sm[f])):\n",
    "#                 sm[g].update({all[f][g]:similarity(dict_token[f],dict_sm[f][g])})\n",
    "            \n",
    "#     best = [[(h,i,sm[h][i]) for i in sm[h] if sm[h][i]>=0.63] for h in sm]\n",
    "    \n",
    "    sm = \"\"\n",
    "    for f in range(len(token)):\n",
    "        if f not in idx_dic:\n",
    "            sm += dict_token[f] + \" \"\n",
    "        else:\n",
    "            sm += dict_sm[f][0] + \" \"\n",
    "            \n",
    "    print(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-e22102a23982>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimport_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-161-4ce1bd547152>\u001b[0m in \u001b[0;36mimport_data\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     47\u001b[0m             \u001b[0msm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdict_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m             \u001b[0msm\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdict_sm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "import_data(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
