{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text_round1(text):\n",
    "    '''Make text lowercase, remove text in square brackets, remove punctuation and remove words containing numbers.'''\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "round1 = lambda x: clean_text_round1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_round2(text):\n",
    "    '''Get rid of some additional punctuation and non-sensical text that was missed the first time around.'''\n",
    "    text = re.sub('[‘’“”…]', '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    return text\n",
    "\n",
    "round2 = lambda x: clean_text_round2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>distractor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>meals can be served</td>\n",
       "      <td>in rooms at  p m</td>\n",
       "      <td>outside the room at  p m in the dining  room a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>it can be inferred from the passage that</td>\n",
       "      <td>the local government can deal with the problem...</td>\n",
       "      <td>if some tragedies occur again   relevant depar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the author called tommy s parents in order to</td>\n",
       "      <td>help them realize their influence on tommy</td>\n",
       "      <td>blame tommy for his failing grades blame tommy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>it can be inferred from the passage that</td>\n",
       "      <td>the writer is not very willing to use idioms</td>\n",
       "      <td>idioms are the most important part in a langua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>how can we deal with snake wounds according to...</td>\n",
       "      <td>stay calm and do nt move</td>\n",
       "      <td>cut the wound and suck the poison out</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "0                                meals can be served   \n",
       "1           it can be inferred from the passage that   \n",
       "2      the author called tommy s parents in order to   \n",
       "3           it can be inferred from the passage that   \n",
       "4  how can we deal with snake wounds according to...   \n",
       "\n",
       "                                         answer_text  \\\n",
       "0                                   in rooms at  p m   \n",
       "1  the local government can deal with the problem...   \n",
       "2         help them realize their influence on tommy   \n",
       "3       the writer is not very willing to use idioms   \n",
       "4                          stay calm and do nt move    \n",
       "\n",
       "                                          distractor  \n",
       "0  outside the room at  p m in the dining  room a...  \n",
       "1  if some tragedies occur again   relevant depar...  \n",
       "2  blame tommy for his failing grades blame tommy...  \n",
       "3  idioms are the most important part in a langua...  \n",
       "4             cut the wound and suck the poison out   "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[['question']]= pd.DataFrame(dataset.question.apply(round1))\n",
    "dataset[['answer_text']]= pd.DataFrame(dataset.answer_text.apply(round1))\n",
    "dataset[['distractor']]= pd.DataFrame(dataset.distractor.apply(round1))\n",
    "dataset[['question']]= pd.DataFrame(dataset.question.apply(round2))\n",
    "dataset[['answer_text']]= pd.DataFrame(dataset.answer_text.apply(round2))\n",
    "dataset[['distractor']]= pd.DataFrame(dataset.distractor.apply(round2))\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample['question'].values.tolist()\n",
    "y = sample['answer_text'].values.tolist()\n",
    "z = sample['distractor'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = x+y+z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total_cor = [nltk.word_tokenize(sent) for sent in total]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_all = []\n",
    "for z in total:\n",
    "    total_all.append(z.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL Generated from train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(total_all, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR SIMILARITY CHECK(%)\n",
    "# def similarity(word1,word2):\n",
    "#     if (wordnet.synsets(word1)):\n",
    "#         syn1 = (wordnet.synsets(word1))[0]\n",
    "#     else:\n",
    "#         return 0\n",
    "#     if (wordnet.synsets(word2)):\n",
    "#         syn2 = (wordnet.synsets(word2))[0]\n",
    "#     else:\n",
    "#         return 0\n",
    "    \n",
    "#     return syn1.wup_similarity(syn2)\n",
    "def similarity(word1,word2):\n",
    "    return model.wv.similarity(word1,word2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('Results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wanted pattern        \n",
    "pattern = re.compile(r'^N\\w|V\\w|N\\w\\w|V\\w\\w|J\\w|J\\w\\w|R\\w|R\\w\\w')\n",
    "def token_tag(word_tag):\n",
    "    sample= {}\n",
    "    idx = []\n",
    "    result = []\n",
    "    pos = dict(word_tag)\n",
    "    for s,t in enumerate(pos.values()):\n",
    "        if pattern.match(t):\n",
    "            idx.append(s)\n",
    "    for s in idx:\n",
    "        result.append(word_tag[s][0])\n",
    "    return result,idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_data(df):\n",
    "    answer_text = df[['answer_text']].values.tolist()\n",
    "    l = 2\n",
    "    distractors = []\n",
    "    for a in answer_text:\n",
    "        individual = word_tokenize(str(' '.join(a)))\n",
    "        dict_s = {}\n",
    "        for b,c in enumerate(individual):\n",
    "            dict_s[b] = c\n",
    "        token = word_tokenize(str(' '.join(a)))\n",
    "        token_pos = nltk.pos_tag(token)\n",
    "        all,idx = token_tag(token_pos)\n",
    "        all = [[x] for x in all]\n",
    "#         print(all)\n",
    "        try:\n",
    "            sm = []\n",
    "            for d in all:\n",
    "                model_dict = dict(model.wv.most_similar(d))\n",
    "                sm.append([e for e in model_dict.keys() if model_dict[e] >= 0.75])\n",
    "            idx_dic = []\n",
    "            sm = defaultdict(dict)\n",
    "            for f in (dict_s.keys()):\n",
    "                if (f not in idx):\n",
    "                    idx_dic.append(f)\n",
    "                else:\n",
    "                    for g in range(len(all[f])):\n",
    "                        sm[g].update({all[f][g]:similarity(dict_s[f],all[f][g])})\n",
    "                        \n",
    "                        \n",
    "            best = [[(h,i,sm[h][i]) for i in sm[h] if sm[h][i]>=0.63] for h in sm]\n",
    "            print(l,best)\n",
    "\n",
    "\n",
    "            \n",
    "            default_s = dict_s\n",
    "            \n",
    "#             global all_str\n",
    "            all_str = []\n",
    "            \n",
    "            \n",
    "            for j in range(len(best)):\n",
    "                for k in range(len(best[j])):\n",
    "                    default_s[best[j][k][0]] = best[j][k][1]\n",
    "                    all_str.append(' '.join(list(default_s.values())))\n",
    "                    \n",
    "#             distractors[l] = ','.join(all_str[:3])\n",
    "            l += 1\n",
    "#             print(l,all_str)\n",
    "            \n",
    "        except Exception as e:\n",
    "            pass\n",
    "#     print(distractors)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 [[(0, 'spoken', 1.0), (0, 'highly', 1.0)]]\n",
      "3 [[(0, 'thermometer', 0.99999994)]]\n",
      "4 [[(0, 'probably', 1.0), (0, 'impress', 1.0), (0, 'participants', 1.0), (0, 'deeply', 1.0)]]\n",
      "5 [[(0, 'doctor', 1.0)]]\n",
      "6 [[(0, 'rivers', 1.0)]]\n",
      "7 [[(0, 'too', 1.0), (0, 'much', 1.0), (0, 'homework', 1.0)]]\n",
      "8 [[(0, 'mass', 1.0), (0, 'extinction', 1.0)]]\n",
      "9 [[(0, 'harmful', 1.0), (0, 'chemicals', 1.0)]]\n",
      "10 [[(0, 'old', 1.0), (0, 'people', 1.0)]]\n",
      "11 [[(0, 'stop', 1.0), (0, 'wasting', 1.0), (0, 'food', 1.0)]]\n",
      "12 [[(0, 'sharks', 1.0), (0, 'are', 0.99999994), (0, 'dangerous', 1.0), (0, 'animals', 1.0)]]\n",
      "13 [[(0, 'faces', 1.0), (0, 'danger', 0.99999994)]]\n",
      "14 [[(0, 'more', 1.0), (0, 'people', 1.0)]]\n",
      "15 [[(0, 'dinner', 1.0)]]\n",
      "16 [[(0, 'throwing', 1.0), (0, 'stones', 1.0), (0, 'often', 1.0), (0, 'caused', 1.0), (0, 'injuries', 1.0)]]\n",
      "17 [[(0, 'new', 1.0), (0, 'information', 1.0), (0, 'brings', 0.99999994)]]\n",
      "18 [[(0, 'lives', 1.0), (0, 'alone', 1.0)]]\n",
      "19 [[(0, 'picnics', 1.0)]]\n",
      "20 [[(0, 'young', 1.0), (0, 'children', 0.99999994)]]\n",
      "21 [[(0, 'heart', 1.0), (0, 'disease', 1.0)]]\n",
      "22 [[(0, 'control', 1.0), (0, 'room', 1.0), (0, 'temperature', 1.0)]]\n",
      "23 [[(0, 'flowers', 1.0)]]\n",
      "24 [[(0, 'met', 1.0), (0, 'financial', 1.0), (0, 'problems', 1.0), (0, 'recently', 1.0)]]\n",
      "25 [[(0, 'love', 1.0)]]\n",
      "26 [[(0, 'pay', 1.0), (0, 'more', 1.0), (0, 'insurance', 1.0)]]\n",
      "27 [[(0, 'improving', 1.0), (0, 'working', 1.0000001), (0, 'memory', 1.0)]]\n",
      "28 [[(0, 'get', 1.0), (0, 'sick', 1.0)]]\n",
      "29 [[(0, 'something', 1.0), (0, 'is', 1.0), (0, 'wrong', 1.0)]]\n",
      "30 [[(0, 'emotional', 1.0), (0, 'damage', 1.0)]]\n",
      "31 [[(0, 'was', 1.0), (0, 'driving', 0.99999994), (0, 'slowly', 1.0)]]\n",
      "32 [[(0, 'good', 1.0), (0, 'manners', 0.99999994), (0, 'are', 0.99999994), (0, 'disappearing', 0.99999994), (0, 'little', 1.0)]]\n",
      "33 [[(0, 'play', 1.0), (0, 'tennis', 1.0)]]\n",
      "34 [[(0, 'high', 0.9999999), (0, 'temperature', 1.0), (0, 'helps', 1.0), (0, 'bacteria', 1.0), (0, 'grow', 1.0), (0, 'faster', 1.0)]]\n",
      "35 [[(0, 'are', 0.99999994), (0, 'widely', 1.0), (0, 'used', 1.0), (0, 'now', 1.0)]]\n",
      "36 [[(0, 'notice', 1.0)]]\n",
      "37 [[(0, 'escape', 1.0)]]\n",
      "38 [[(0, 'study', 0.99999994), (0, 'mathematics', 1.0)]]\n",
      "39 [[(0, 'continuous', 1.0), (0, 'small', 0.99999994), (0, 'classes', 1.0), (0, 'help', 1.0), (0, 'students', 1.0), (0, 'achieve', 1.0), (0, 'more', 1.0)]]\n",
      "40 [[(0, 'boarding', 1.0), (0, 'students', 1.0)]]\n",
      "41 [[(0, 'students', 1.0)]]\n",
      "42 []\n",
      "43 [[(0, 'make', 1.0), (0, 'fewer', 1.0), (0, 'accident', 1.0)]]\n",
      "44 [[(0, 'building', 1.0), (0, 'more', 1.0), (0, 'department', 1.0), (0, 'stores', 1.0)]]\n",
      "45 [[(0, 'natural', 1.0), (0, 'killer', 1.0), (0, 'cells', 1.0)]]\n",
      "46 [[(0, 'nature', 1.0)]]\n",
      "47 [[(0, 'learn', 1.0)]]\n",
      "48 [[(0, 'be', 1.0), (0, 'getting', 1.0), (0, 'colder', 1.0)]]\n",
      "49 [[(0, 'died', 1.0)]]\n",
      "50 [[(0, 'write', 1.0), (0, 'more', 1.0), (0, 'notes', 0.99999994)]]\n",
      "51 [[(0, 'foreign', 1.0), (0, 'language', 1.0)]]\n",
      "52 [[(0, 'coffee', 1.0)]]\n",
      "53 [[(0, 'spending', 1.0), (0, 'nature', 1.0)]]\n",
      "54 [[(0, 'informative', 1.0), (0, 'speeches', 1.0)]]\n",
      "55 [[(0, 'appreciate', 1.0), (0, 'such', 0.99999994), (0, 'learning', 1.0), (0, 'opportunities', 1.0)]]\n",
      "56 [[(0, 'rapid', 1.0), (0, 'technological', 1.0), (0, 'advance', 1.0)]]\n",
      "57 [[(0, 'send', 1.0), (0, 'information', 1.0)]]\n",
      "58 [[(0, 'rescue', 1.0), (0, 'disappearing', 0.99999994), (0, 'languages', 1.0)]]\n",
      "59 [[(0, 'earth', 1.0)]]\n",
      "60 [[(0, 'gain', 1.0), (0, 'more', 1.0), (0, 'weight', 1.0)]]\n",
      "61 [[(0, 'dead', 1.0), (0, 'leaves', 1.0)]]\n",
      "62 [[(0, 'parents', 1.0)]]\n",
      "63 [[(0, 'writing', 1.0), (0, 'poems', 1.0)]]\n",
      "64 [[(0, 'landlord', 1.0)]]\n",
      "65 [[(0, 'making', 1.0), (0, 'boats', 1.0), (0, 'is', 1.0), (0, 'difficult', 1.0)]]\n",
      "66 [[(0, 'fishermen', 1.0)]]\n",
      "67 [[(0, 'tiger', 1.0)]]\n",
      "68 [[(0, 'public', 1.0), (0, 'schools', 1.0)]]\n",
      "69 [[(0, 'money', 1.0)]]\n",
      "70 [[(0, 'likes', 1.0), (0, 'blue', 1.0), (0, 'pants', 1.0), (0, 'very', 1.0), (0, 'much', 1.0)]]\n",
      "71 [[(0, 'animal', 1.0), (0, 'shelter', 1.0)]]\n",
      "72 [[(0, 'skillful', 1.0), (0, 'employees', 1.0)]]\n",
      "73 [[(0, 'climate', 0.99999994), (0, 'change', 1.0)]]\n",
      "74 [[(0, 'sometimes', 1.0), (0, 'animals', 1.0), (0, 'killed', 1.0), (0, 'men', 1.0)]]\n",
      "75 [[(0, 'classical', 1.0), (0, 'music', 1.0)]]\n",
      "76 [[(0, 'social', 1.0), (0, 'companionship', 1.0)]]\n",
      "77 [[(0, 'mother', 1.0)]]\n",
      "78 [[(0, 'exercise', 1.0), (0, 'regularly', 1.0)]]\n",
      "79 [[(0, 'love', 1.0), (0, 'never', 1.0), (0, 'really', 1.0), (0, 'dies', 1.0)]]\n",
      "80 [[(0, 'showed', 1.0), (0, 'little', 1.0), (0, 'international', 1.0), (0, 'friendship', 1.0)]]\n",
      "81 [[(0, 'random', 1.0), (0, 'students', 1.0)]]\n",
      "82 [[(0, 'was', 1.0), (0, 'busy', 1.0), (0, 'solving', 1.0), (0, 'other', 1.0), (0, 'more', 1.0), (0, 'serious', 1.0), (0, 'economic', 1.0), (0, 'problems', 1.0)]]\n",
      "83 [[(0, 'thought', 1.0), (0, 'laptops', 1.0), (0, 'were', 1.0), (0, 'too', 1.0), (0, 'heavy', 1.0)]]\n",
      "84 [[(0, 'be', 1.0), (0, 'more', 1.0), (0, 'healthy', 1.0)]]\n",
      "85 [[(0, 'had', 1.0), (0, 'great', 1.0), (0, 'difficulty', 1.0), (0, 'looking', 1.0)]]\n",
      "86 [[(0, 'changing', 1.0), (0, 'weight', 1.0)]]\n",
      "87 [[(0, 'reducing', 1.0), (0, 'too', 1.0), (0, 'much', 1.0), (0, 'fossil', 1.0), (0, 'fuel', 0.99999994)]]\n",
      "88 []\n",
      "89 [[(0, 'gun', 0.99999994), (0, 'murders', 1.0)]]\n",
      "90 [[(0, 'kill', 1.0), (0, 'enemies', 1.0)]]\n",
      "91 [[(0, 'robots', 0.99999994), (0, 'are', 0.99999994), (0, 'very', 1.0), (0, 'popular', 1.0)]]\n",
      "92 [[(0, 'caused', 1.0), (0, 'great', 1.0), (0, 'economic', 1.0), (0, 'losses', 1.0)]]\n",
      "93 [[(0, 'reduce', 1.0), (0, 'pollution', 1.0)]]\n",
      "94 [[(0, 'buy', 1.0), (0, 'pet', 1.0), (0, 'food', 1.0)]]\n",
      "95 [[(0, 'help', 1.0), (0, 'stop', 1.0), (0, 'global', 1.0), (0, 'warming', 1.0)]]\n",
      "96 [[(0, 'making', 1.0), (0, 'employees', 1.0), (0, 'more', 1.0), (0, 'attractive', 1.0)]]\n",
      "97 [[(0, 'promote', 1.0), (0, 'ethical', 1.0), (0, 'destinations', 1.0)]]\n",
      "98 [[(0, 'smoking', 1.0)]]\n",
      "99 [[(0, 'cause', 1.0), (0, 'traffic', 0.99999994), (0, 'jams', 1.0)]]\n",
      "100 [[(0, 'girls', 1.0), (0, 'have', 1.0), (0, 'entered', 1.0), (0, 'colleges', 1.0)]]\n",
      "101 [[(0, 'high', 0.9999999), (0, 'fashion', 1.0)]]\n",
      "102 [[(0, 'master', 1.0)]]\n",
      "103 [[(0, 'use', 1.0), (0, 'mobile', 0.99999994), (0, 'phones', 1.0), (0, 'too', 1.0), (0, 'much', 1.0)]]\n",
      "104 [[(0, 'help', 1.0), (0, 'businessmen', 1.0), (0, 'get', 1.0), (0, 'more', 1.0), (0, 'money', 1.0)]]\n",
      "105 [[(0, 'understand', 1.0), (0, 'designing', 1.0), (0, 'better', 0.99999994)]]\n",
      "106 [[(0, 'noise', 1.0), (0, 'pollution', 1.0)]]\n",
      "107 [[(0, 'keep', 1.0), (0, 'thinking', 1.0), (0, 'hard', 1.0)]]\n",
      "108 [[(0, 'become', 1.0), (0, 'extinct', 1.0)]]\n",
      "109 [[(0, 'only', 1.0), (0, 'essay', 1.0), (0, 'questions', 1.0)]]\n",
      "110 [[(0, 'studied', 1.0), (0, 'very', 1.0), (0, 'hard', 1.0)]]\n",
      "111 [[(0, 'people', 1.0), (0, 'are', 0.99999994), (0, 'more', 1.0), (0, 'easily', 1.0), (0, 'influenced', 1.0)]]\n",
      "112 [[(0, 'watch', 1.0), (0, 'animals', 1.0)]]\n",
      "113 [[(0, 'saying', 1.0), (0, 'things', 1.0), (0, 'out', 1.0), (0, 'aloud', 1.0)]]\n",
      "114 [[(0, 'be', 1.0), (0, 'kind', 0.99999994)]]\n",
      "115 [[(0, 'sports', 1.0), (0, 'hall', 1.0), (0, 'exercise', 1.0)]]\n",
      "116 [[(0, 'assimilate', 1.0), (0, 'only', 1.0), (0, 'partially', 1.0)]]\n",
      "117 [[(0, 'available', 1.0), (0, 'panda', 1.0), (0, 'eggs', 1.0)]]\n",
      "118 [[(0, 'avoid', 1.0), (0, 'direct', 1.0), (0, 'conflicts', 1.0)]]\n",
      "119 [[(0, 'technology', 1.0)]]\n",
      "120 [[(0, 'alternative', 1.0)]]\n",
      "121 [[(0, 'treats', 1.0), (0, 'sex', 1.0), (0, 'difference', 1.0), (0, 'objectively', 1.0)]]\n",
      "122 [[(0, 'drops', 0.99999994), (0, 'just', 1.0)]]\n",
      "123 [[(0, 'creativity', 1.0)]]\n",
      "124 [[(0, 'developing', 0.99999994), (0, 'software', 1.0)]]\n",
      "125 [[(0, 'slow', 1.0), (0, 'movement', 0.99999994)]]\n",
      "126 [[(0, 'not', 1.0), (0, 'have', 1.0), (0, 'enough', 1.0), (0, 'income', 1.0)]]\n",
      "127 [[(0, 'prevent', 1.0), (0, 'car', 1.0), (0, 'accidents', 1.0)]]\n",
      "128 [[(0, 'make', 1.0), (0, 'electricity', 1.0)]]\n",
      "129 [[(0, 'many', 1.0), (0, 'families', 1.0), (0, 'were', 1.0), (0, 'using', 1.0), (0, 'lamps', 1.0)]]\n",
      "130 [[(0, 'dinner', 1.0), (0, 'guests', 1.0)]]\n",
      "131 [[(0, 'promote', 1.0), (0, 'global', 1.0), (0, 'economic', 1.0), (0, 'growth', 1.0)]]\n",
      "132 [[(0, 'lung', 1.0), (0, 'cancer', 0.9999999)]]\n",
      "133 [[(0, 'felt', 1.0), (0, 'ashamed', 1.0)]]\n",
      "134 [[(0, 'nobody', 1.0), (0, 'was', 1.0), (0, 'interested', 0.99999994)]]\n",
      "135 [[(0, 'find', 1.0), (0, 'relationships', 1.0)]]\n",
      "136 [[(0, 'rhino', 1.0), (0, 'numbers', 1.0), (0, 'have', 1.0), (0, 'declined', 1.0), (0, 'greatly', 1.0)]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 [[(0, 'something', 1.0), (0, 'has', 1.0), (0, 'changed', 0.99999994)]]\n",
      "138 [[(0, 'fast', 1.0), (0, 'rate', 1.0)]]\n",
      "139 [[(0, 'conventional', 1.0), (0, 'playtime', 1.0)]]\n",
      "140 [[(0, 'customs', 1.0)]]\n",
      "141 []\n",
      "142 []\n",
      "143 [[(0, 'sleep', 1.0)]]\n",
      "144 [[(0, 'avoiding', 1.0), (0, 'driving', 0.99999994), (0, 'cars', 1.0)]]\n",
      "145 [[(0, 'keep', 1.0), (0, 'down', 1.0), (0, 'noise', 1.0)]]\n",
      "146 [[(0, 'food', 1.0), (0, 'service', 1.0), (0, 'assistant', 1.0)]]\n",
      "147 [[(0, 'liked', 1.0), (0, 'being', 1.0), (0, 'told', 1.0), (0, 'lies', 1.0)]]\n",
      "148 [[(0, 'finding', 1.0), (0, 'toys', 1.0)]]\n",
      "149 [[(0, 'keeping', 0.99999994), (0, 'close', 1.0), (0, 'enough', 1.0), (0, 'is', 1.0), (0, 'preferred', 1.0)]]\n",
      "150 [[(0, 'junk', 1.0), (0, 'sleep', 1.0)]]\n",
      "151 [[(0, 'stomach', 1.0), (0, 'diseases', 1.0)]]\n",
      "152 [[(0, 'show', 1.0), (0, 'interest', 1.0)]]\n",
      "153 [[(0, 'folic', 1.0), (0, 'acid', 1.0)]]\n",
      "154 [[(0, 'has', 1.0), (0, 'limited', 1.0), (0, 'freedom', 1.0)]]\n",
      "155 [[(0, 'nuclear', 1.0), (0, 'radiation', 1.0)]]\n",
      "156 [[(0, 'overcome', 1.0), (0, 'tough', 1.0), (0, 'problems', 1.0)]]\n",
      "157 [[(0, 'worship', 1.0), (0, 'money', 1.0)]]\n",
      "158 [[(0, 'forms', 1.0), (0, 'very', 1.0), (0, 'slowly', 1.0)]]\n",
      "159 [[(0, 'attitudes', 1.0), (0, 'towards', 1.0), (0, 'life', 0.99999994)]]\n",
      "160 [[(0, 'habitat', 1.0), (0, 'protection', 1.0)]]\n",
      "161 [[(0, 'parents', 1.0)]]\n",
      "162 [[(0, 'accept', 1.0), (0, 'negative', 1.0), (0, 'values', 1.0), (0, 'more', 1.0), (0, 'quickly', 1.0)]]\n",
      "163 [[(0, 'pigs', 1.0), (0, 'need', 1.0), (0, 'affection', 1.0)]]\n",
      "164 [[(0, 'work', 0.99999994), (0, 'long', 1.0), (0, 'hours', 0.99999994)]]\n",
      "165 [[(0, 'set', 1.0), (0, 'challenging', 1.0), (0, 'goals', 1.0)]]\n",
      "166 [[(0, 'has', 1.0), (0, 'fine', 0.99999994), (0, 'weather', 1.0)]]\n",
      "167 [[(0, 'unfair', 1.0)]]\n",
      "168 [[(0, 'doing', 1.0), (0, 'little', 1.0), (0, 'physical', 1.0), (0, 'activity', 1.0)]]\n",
      "169 []\n",
      "170 [[(0, 'allowing', 1.0), (0, 'video', 1.0), (0, 'chat', 1.0)]]\n",
      "171 [[(0, 'pink', 1.0), (0, 'carnations', 1.0)]]\n",
      "172 [[(0, 'human', 1.0), (0, 'communication', 1.0), (0, 'means', 1.0), (0, 'information', 1.0), (0, 'exchange', 0.99999994)]]\n",
      "173 [[(0, 'were', 1.0), (0, 'more', 1.0), (0, 'anxious', 1.0)]]\n",
      "174 [[(0, 'clever', 1.0), (0, 'lamb', 0.99999994)]]\n",
      "175 [[(0, 'hands', 0.99999994), (0, 'down', 1.0)]]\n",
      "176 [[(0, 'giving', 1.0), (0, 'smile', 1.0), (0, 'cards', 1.0)]]\n",
      "177 [[(0, 'foreign', 1.0), (0, 'children', 0.99999994), (0, 'aged', 0.99999994)]]\n",
      "178 [[(0, 'maglev', 1.0)]]\n",
      "179 [[(0, 'teenagers', 0.99999994)]]\n",
      "180 [[(0, 'treat', 1.0), (0, 'sick', 1.0), (0, 'children', 0.99999994)]]\n",
      "181 [[(0, 'work', 0.99999994), (0, 'hard', 1.0)]]\n",
      "182 [[(0, 'sells', 1.0), (0, 'medicine', 1.0)]]\n",
      "183 []\n",
      "184 [[(0, 'senior', 0.99999994), (0, 'high', 0.9999999), (0, 'students', 1.0)]]\n",
      "185 [[(0, 'sleepaway', 1.0), (0, 'camp', 0.99999994), (0, 'is', 1.0), (0, 'more', 1.0), (0, 'exciting', 1.0)]]\n",
      "186 [[(0, 'dolphins', 1.0)]]\n",
      "187 [[(0, 'eat', 1.0), (0, 'much', 1.0), (0, 'meat', 1.0)]]\n",
      "188 [[(0, 'not', 1.0), (0, 'yet', 1.0), (0, 'using', 1.0), (0, 'stamps', 1.0)]]\n",
      "189 [[(0, 'caused', 1.0), (0, 'chemical', 1.0), (0, 'changes', 1.0)]]\n",
      "190 [[(0, 'love', 1.0), (0, 'animals', 1.0)]]\n",
      "191 [[(0, 'ticket', 1.0)]]\n",
      "192 [[(0, 'cough', 1.0)]]\n",
      "193 [[(0, 'cleaner', 1.0)]]\n",
      "194 [[(0, 'basketball', 1.0), (0, 'players', 1.0)]]\n",
      "195 [[(0, 'make', 1.0), (0, 'less', 1.0), (0, 'pollution', 1.0)]]\n",
      "196 [[(0, 'show', 1.0), (0, 'appreciation', 1.0)]]\n",
      "197 [[(0, 'commuting', 1.0), (0, 'causes', 1.0), (0, 'ill', 1.0), (0, 'health', 1.0)]]\n",
      "198 [[(0, 'robots', 0.99999994), (0, 'are', 0.99999994)]]\n",
      "199 [[(0, 'effective', 1.0), (0, 'measures', 1.0), (0, 'have', 1.0), (0, 'not', 1.0), (0, 'been', 1.0), (0, 'taken', 1.0)]]\n",
      "200 [[(0, 'create', 1.0), (0, 'much', 1.0), (0, 'smaller', 1.0), (0, 'computers', 1.0)]]\n",
      "201 [[(0, 'be', 1.0), (0, 'friendly', 1.0)]]\n",
      "202 [[(0, 'has', 1.0), (0, 'sixteen', 1.0), (0, 'toes', 1.0), (0, 'altogether', 1.0)]]\n",
      "203 [[(0, 'ocean', 1.0), (0, 'ecosystems', 1.0), (0, 'face', 1.0), (0, 'more', 1.0), (0, 'dangers', 1.0)]]\n",
      "204 [[(0, 'doing', 1.0), (0, 'pet', 1.0), (0, 'first', 1.0), (0, 'aid', 1.0)]]\n",
      "205 [[(0, 'memorable', 1.0), (0, 'phrases', 1.0)]]\n",
      "206 [[(0, 'verse', 1.0)]]\n",
      "207 [[(0, 'aging', 1.0)]]\n",
      "208 [[(0, 'money', 1.0), (0, 'does', 1.0), (0, 'not', 1.0), (0, 'surely', 1.0), (0, 'mean', 1.0), (0, 'happiness', 1.0)]]\n",
      "209 [[(0, 'future', 1.0), (0, 'travel', 1.0)]]\n",
      "210 [[(0, 'protect', 1.0), (0, 'animals', 1.0)]]\n",
      "211 [[(0, 'working', 1.0000001), (0, 'businessmen', 1.0)]]\n",
      "212 [[(0, 'having', 1.0), (0, 'mental', 1.0), (0, 'problems', 1.0)]]\n",
      "213 [[(0, 'testing', 1.0), (0, 'families', 1.0)]]\n",
      "214 [[(0, 'very', 1.0), (0, 'tiring', 1.0)]]\n",
      "215 [[(0, 'knew', 1.0), (0, 'something', 1.0), (0, 'special', 0.99999994)]]\n",
      "216 [[(0, 'express', 1.0)]]\n",
      "217 [[(0, 'buying', 1.0), (0, 'guns', 1.0), (0, 'is', 1.0), (0, 'very', 1.0), (0, 'hard', 1.0)]]\n",
      "218 [[(0, 'noisy', 1.0)]]\n",
      "219 [[(0, 'getting', 1.0), (0, 'examination', 0.99999994), (0, 'system', 1.0), (0, 'improved', 1.0)]]\n",
      "220 [[(0, 'dolls', 1.0)]]\n",
      "221 [[(0, 'fight', 1.0), (0, 'back', 1.0), (0, 'immediately', 1.0)]]\n",
      "222 [[(0, 'raising', 1.0), (0, 'eyebrows', 1.0)]]\n",
      "223 [[(0, 'many', 1.0), (0, 'plates', 1.0), (0, 'met', 1.0)]]\n",
      "224 [[(0, 'appear', 0.99999994), (0, 'knowledgeable', 1.0)]]\n",
      "225 [[(0, 'night', 1.0)]]\n",
      "226 [[(0, 'make', 1.0), (0, 'other', 1.0), (0, 'ones', 1.0)]]\n",
      "227 [[(0, 'clothes', 1.0), (0, 'have', 1.0), (0, 'extraordinary', 1.0), (0, 'power', 0.99999994)]]\n",
      "228 [[(0, 'positive', 1.0), (0, 'action', 1.0)]]\n",
      "229 [[(0, 'exemplify', 1.0), (0, 'human', 1.0), (0, 'brain', 1.0), (0, 'disease', 1.0)]]\n",
      "230 [[(0, 'proper', 1.0), (0, 'packaging', 1.0), (0, 'makes', 1.0), (0, 'people', 1.0), (0, 'attractive', 1.0)]]\n",
      "231 [[(0, 'successful', 1.0)]]\n",
      "232 [[(0, 'wasting', 1.0), (0, 'wrapping', 1.0), (0, 'paper', 1.0)]]\n",
      "233 [[(0, 'be', 1.0), (0, 'independent', 1.0)]]\n",
      "234 [[(0, 'clothesline', 1.0), (0, 'drying', 0.99999994), (0, 'reduces', 1.0), (0, 'home', 1.0), (0, 'value', 1.0)]]\n",
      "235 [[(0, 'become', 1.0), (0, 'more', 1.0), (0, 'anxious', 1.0)]]\n",
      "236 [[(0, 'becomes', 1.0), (0, 'active', 1.0), (0, 'sometimes', 1.0)]]\n",
      "237 [[(0, 'loves', 0.99999994), (0, 'reading', 1.0)]]\n",
      "238 []\n",
      "239 [[(0, 'likes', 1.0), (0, 'music', 1.0), (0, 'very', 1.0), (0, 'much', 1.0)]]\n",
      "240 [[(0, 'many', 1.0), (0, 'illegal', 1.0), (0, 'protesters', 1.0), (0, 'were', 1.0), (0, 'arrested', 1.0)]]\n",
      "241 [[(0, 'come', 1.0), (0, 'home', 1.0), (0, 'late', 1.0)]]\n",
      "242 [[(0, 'down', 1.0), (0, 'jacket', 1.0)]]\n",
      "243 [[(0, 'seeking', 1.0), (0, 'creative', 0.99999994), (0, 'solutions', 1.0)]]\n",
      "244 [[(0, 'not', 1.0), (0, 'quite', 1.0), (0, 'helpful', 1.0)]]\n",
      "245 [[(0, 'detecting', 1.0), (0, 'oxygen', 1.0)]]\n",
      "246 []\n",
      "247 [[(0, 'man', 1.0)]]\n",
      "248 [[(0, 'single', 1.0), (0, 'rooms', 1.0)]]\n",
      "249 [[(0, 'curing', 1.0), (0, 'heart', 1.0), (0, 'disease', 1.0)]]\n",
      "250 [[(0, 'having', 1.0), (0, 'fun', 0.99999994)]]\n",
      "251 [[(0, 'bedroom', 1.0)]]\n",
      "252 [[(0, 'buy', 1.0), (0, 'high', 0.9999999), (0, 'quality', 1.0), (0, 'flowers', 1.0)]]\n",
      "253 [[(0, 'parental', 1.0), (0, 'education', 1.0)]]\n",
      "254 [[(0, 'play', 1.0), (0, 'computer', 1.0), (0, 'games', 1.0)]]\n",
      "255 [[(0, 'have', 1.0), (0, 'more', 1.0), (0, 'visiting', 1.0), (0, 'time', 0.99999994)]]\n",
      "256 [[(0, 'respecting', 0.99999994), (0, 'others', 1.0), (0, 'means', 1.0)]]\n",
      "257 [[(0, 'playing', 1.0), (0, 'computer', 1.0), (0, 'games', 1.0)]]\n",
      "258 [[(0, 'was', 1.0), (0, 'clever', 1.0)]]\n",
      "259 [[(0, 'cheaper', 1.0), (0, 'goods', 1.0)]]\n",
      "260 [[(0, 'following', 1.0), (0, 'certain', 1.0), (0, 'rules', 1.0)]]\n",
      "261 [[(0, 'mainly', 1.0)]]\n",
      "262 [[(0, 'fast', 1.0), (0, 'food', 1.0)]]\n",
      "263 [[(0, 'public', 1.0), (0, 'figures', 1.0), (0, 'know', 1.0), (0, 'hugging', 1.0), (0, 'functions', 1.0), (0, 'well', 1.0)]]\n",
      "264 [[(0, 'make', 1.0), (0, 'less', 1.0), (0, 'pollution', 1.0)]]\n",
      "265 [[(0, 'ocean', 1.0), (0, 'cleaners', 1.0)]]\n",
      "266 [[(0, 'morning', 1.0), (0, 'programs', 1.0)]]\n",
      "267 [[(0, 'young', 1.0), (0, 'people', 1.0)]]\n",
      "268 [[(0, 'gain', 1.0), (0, 'technical', 1.0), (0, 'knowledge', 1.0)]]\n",
      "269 [[(0, 'fat', 1.0)]]\n",
      "270 [[(0, 'poor', 1.0), (0, 'immigrants', 1.0), (0, 'needed', 1.0), (0, 'help', 1.0), (0, 'badly', 1.0)]]\n",
      "271 [[(0, 'different', 1.0), (0, 'animals', 1.0)]]\n",
      "272 [[(0, 'hard', 1.0), (0, 'life', 0.99999994)]]\n",
      "273 [[(0, 'limiting', 1.0), (0, 'heat', 1.0), (0, 'movement', 0.99999994)]]\n",
      "274 [[(0, 'age', 1.0)]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "275 [[(0, 'received', 1.0), (0, 'good', 1.0), (0, 'service', 1.0)]]\n",
      "276 [[(0, 'mailed', 1.0), (0, 'programmes', 1.0)]]\n",
      "277 [[(0, 'unreal', 0.99999994), (0, 'drugs', 1.0)]]\n",
      "278 [[(0, 'read', 1.0), (0, 'international', 1.0), (0, 'business', 1.0), (0, 'newspapers', 1.0)]]\n",
      "279 [[(0, 'choose', 1.0), (0, 'useful', 1.0), (0, 'classes', 1.0)]]\n",
      "280 [[(0, 'die', 1.0), (0, 'earlier', 1.0)]]\n",
      "281 [[(0, 'read', 1.0), (0, 'books', 1.0)]]\n",
      "282 [[(0, 'glaciers', 1.0), (0, 'are', 0.99999994), (0, 'destructive', 1.0)]]\n",
      "283 [[(0, 'prevent', 1.0), (0, 'illegal', 1.0), (0, 'business', 1.0)]]\n",
      "284 [[(0, 'ancient', 0.99999994), (0, 'forests', 1.0)]]\n",
      "285 [[(0, 'favorite', 1.0), (0, 'spring', 1.0), (0, 'activities', 1.0)]]\n",
      "286 [[(0, 'sign', 1.0), (0, 'language', 1.0)]]\n",
      "287 [[(0, 'exercise', 1.0), (0, 'often', 1.0)]]\n",
      "288 [[(0, 'playing', 1.0), (0, 'football', 1.0)]]\n",
      "289 [[(0, 'schoolmates', 1.0)]]\n",
      "290 [[(0, 'hard', 1.0), (0, 'work', 0.99999994)]]\n",
      "291 [[(0, 'suffered', 0.99999994), (0, 'heavy', 1.0), (0, 'financial', 1.0), (0, 'losses', 1.0)]]\n",
      "292 [[(0, 'pets', 1.0), (0, 'are', 0.99999994), (0, 'very', 1.0), (0, 'important', 1.0)]]\n",
      "293 [[(0, 'helps', 1.0), (0, 'deaf', 1.0), (0, 'people', 1.0)]]\n",
      "294 [[(0, 'became', 1.0), (0, 'very', 1.0), (0, 'sad', 1.0)]]\n",
      "295 [[(0, 'clothes', 1.0), (0, 'cleaner', 1.0)]]\n",
      "296 [[(0, 'man', 1.0), (0, 'has', 1.0), (0, 'helped', 1.0), (0, 'change', 1.0), (0, 'dogs', 0.99999994)]]\n",
      "297 [[(0, 'make', 1.0), (0, 'young', 1.0), (0, 'users', 1.0000001), (0, 'more', 1.0), (0, 'selfish', 1.0)]]\n",
      "298 [[(0, 'being', 1.0), (0, 'fond', 1.0), (0, 'blind', 1.0), (0, 'was', 1.0), (0, 'embarrassing', 1.0)]]\n",
      "299 [[(0, 'begin', 1.0), (0, 'working', 1.0000001), (0, 'immediately', 1.0)]]\n",
      "300 [[(0, 'handling', 1.0), (0, 'budget', 1.0), (0, 'cuts', 1.0)]]\n",
      "301 [[(0, 'schedules', 1.0), (0, 'were', 1.0), (0, 'reliable', 1.0)]]\n",
      "302 [[(0, 'health', 1.0)]]\n",
      "303 [[(0, 'help', 1.0), (0, 'teach', 1.0), (0, 'school', 1.0), (0, 'subjects', 1.0)]]\n",
      "304 [[(0, 'move', 1.0), (0, 'away', 1.0)]]\n",
      "305 [[(0, 'cell', 1.0), (0, 'phones', 1.0), (0, 'do', 1.0), (0, 'not', 1.0), (0, 'necessarily', 1.0), (0, 'bring', 1.0), (0, 'people', 1.0), (0, 'together', 1.0)]]\n",
      "306 [[(0, 'not', 1.0), (0, 'determined', 0.99999994), (0, 'yet', 1.0)]]\n",
      "307 [[(0, 'see', 1.0), (0, 'red', 1.0)]]\n",
      "308 [[(0, 'trick', 1.0), (0, 'evil', 1.0), (0, 'spirits', 1.0)]]\n",
      "309 [[(0, 'let', 1.0), (0, 'customers', 1.0), (0, 'spend', 1.0), (0, 'more', 1.0), (0, 'time', 0.99999994), (0, 'shopping', 1.0)]]\n",
      "310 [[(0, 'true', 1.0), (0, 'friendship', 1.0), (0, 'is', 1.0), (0, 'very', 1.0), (0, 'important', 1.0)]]\n",
      "311 [[(0, 'suggests', 1.0), (0, 'quality', 1.0), (0, 'work', 0.99999994)]]\n",
      "312 [[(0, 'university', 1.0), (0, 'students', 1.0)]]\n",
      "313 [[(0, 'anywhere', 1.0)]]\n",
      "314 [[(0, 'are', 0.99999994), (0, 'learning', 1.0)]]\n",
      "315 [[(0, 'much', 1.0), (0, 'more', 1.0), (0, 'people', 1.0)]]\n",
      "316 [[(0, 'work', 0.99999994), (0, 'out', 1.0), (0, 'marketing', 1.0), (0, 'plans', 1.0)]]\n",
      "317 [[(0, 'too', 1.0), (0, 'frightened', 1.0)]]\n",
      "318 [[(0, 'many', 1.0), (0, 'new', 1.0), (0, 'subjects', 1.0)]]\n",
      "319 [[(0, 'being', 1.0), (0, 'alive', 1.0), (0, 'is', 1.0), (0, 'lucky', 1.0)]]\n",
      "320 [[(0, 'introduce', 1.0), (0, 'promising', 1.0), (0, 'career', 1.0), (0, 'chances', 1.0)]]\n",
      "321 [[(0, 'are', 0.99999994), (0, 'more', 1.0), (0, 'environmentally', 1.0), (0, 'friendly', 1.0)]]\n",
      "322 [[(0, 'fewer', 1.0), (0, 'women', 1.0), (0, 'liars', 0.99999994), (0, 'are', 0.99999994), (0, 'found', 1.0)]]\n",
      "323 [[(0, 'always', 1.0), (0, 'show', 1.0), (0, 'great', 1.0), (0, 'interest', 1.0)]]\n",
      "324 [[(0, 'do', 1.0), (0, 'something', 1.0), (0, 'else', 1.0), (0, 'actively', 1.0)]]\n",
      "325 [[(0, 'immediate', 1.0), (0, 'pleasure', 1.0)]]\n",
      "326 [[(0, 'cat', 1.0)]]\n",
      "327 [[(0, 'white', 1.0), (0, 'cap', 1.0)]]\n",
      "328 [[(0, 'using', 1.0), (0, 'renewable', 1.0), (0, 'energy', 1.0), (0, 'source', 0.99999994)]]\n",
      "329 [[(0, 'international', 1.0), (0, 'mail', 1.0)]]\n",
      "330 [[(0, 'attracts', 1.0), (0, 'many', 1.0), (0, 'migrant', 1.0), (0, 'workers', 1.0)]]\n",
      "331 [[(0, 'conserve', 1.0), (0, 'energy', 1.0)]]\n",
      "332 [[(0, 'fashionable', 1.0), (0, 'things', 1.0), (0, 'are', 0.99999994), (0, 'expensive', 1.0)]]\n",
      "333 [[(0, 'fat', 1.0), (0, 'people', 1.0), (0, 'have', 1.0), (0, 'fewer', 1.0), (0, 'advantages', 1.0)]]\n",
      "334 [[(0, 'using', 1.0), (0, 'computer', 1.0), (0, 'software', 1.0), (0, 'programs', 1.0)]]\n",
      "335 [[(0, 'schools', 1.0), (0, 'are', 0.99999994), (0, 'still', 1.0), (0, 'necessary', 1.0)]]\n",
      "336 [[(0, 'verse', 1.0)]]\n",
      "337 [[(0, 'have', 1.0), (0, 'poorer', 1.0), (0, 'academic', 0.99999994), (0, 'performances', 1.0)]]\n",
      "338 [[(0, 'human', 1.0), (0, 'activity', 1.0)]]\n",
      "339 [[(0, 'avoid', 1.0), (0, 'meeting', 1.0)]]\n",
      "340 [[(0, 'clothing', 1.0), (0, 'store', 1.0)]]\n",
      "341 [[(0, 'more', 1.0), (0, 'responsible', 1.0)]]\n",
      "342 [[(0, 'cultural', 0.99999994), (0, 'differences', 1.0)]]\n",
      "343 [[(0, 'reducing', 1.0), (0, 'heart', 1.0), (0, 'attacks', 1.0)]]\n",
      "344 [[(0, 'unfortunate', 1.0), (0, 'events', 0.99999994), (0, 'usually', 1.0), (0, 'happen', 1.0), (0, 'together', 1.0)]]\n",
      "345 [[(0, 'limiting', 1.0), (0, 'world', 1.0), (0, 'population', 1.0), (0, 'growth', 1.0)]]\n",
      "346 [[(0, 'have', 1.0), (0, 'movable', 1.0), (0, 'structure', 1.0)]]\n",
      "347 [[(0, 'only', 1.0), (0, 'provides', 1.0), (0, 'interesting', 1.0), (0, 'patterns', 1.0)]]\n",
      "348 []\n",
      "349 [[(0, 'big', 1.0), (0, 'scientific', 0.99999994), (0, 'experiments', 1.0)]]\n",
      "350 [[(0, 'breed', 1.0), (0, 'fast', 1.0)]]\n",
      "351 [[(0, 'natural', 1.0), (0, 'gas', 1.0)]]\n",
      "352 [[(0, 'is', 1.0), (0, 'worth', 1.0), (0, 'travelling', 1.0)]]\n",
      "353 [[(0, 'sea', 1.0), (0, 'travel', 1.0), (0, 'provided', 1.0), (0, 'easier', 1.0), (0, 'routes', 1.0)]]\n",
      "354 [[(0, 'help', 1.0), (0, 'children', 0.99999994), (0, 'recognize', 1.0), (0, 'simple', 1.0), (0, 'letters', 0.99999994)]]\n",
      "355 [[(0, 'respect', 1.0)]]\n",
      "356 [[(0, 'uniforms', 1.0)]]\n",
      "357 [[(0, 'eat', 1.0), (0, 'junk', 1.0), (0, 'food', 1.0)]]\n",
      "358 [[(0, 'poetry', 1.0)]]\n",
      "359 [[(0, 'felt', 1.0), (0, 'angry', 1.0)]]\n",
      "360 [[(0, 'take', 1.0), (0, 'care', 1.0)]]\n",
      "361 [[(0, 'have', 1.0), (0, 'outdoor', 1.0), (0, 'activities', 1.0)]]\n",
      "362 [[(0, 'thoroughly', 0.99999994), (0, 'prepare', 1.0)]]\n",
      "363 [[(0, 'teacher', 1.0)]]\n",
      "364 [[(0, 'population', 1.0), (0, 'pollution', 1.0)]]\n",
      "365 [[(0, 'still', 1.0), (0, 'felt', 1.0), (0, 'pain', 1.0)]]\n",
      "366 [[(0, 'animal', 1.0), (0, 'farming', 0.99999994), (0, 'produces', 1.0), (0, 'greenhouse', 1.0), (0, 'gases', 1.0)]]\n",
      "367 [[(0, 'very', 1.0), (0, 'anxious', 1.0)]]\n",
      "368 [[(0, 'living', 1.0), (0, 'things', 1.0), (0, 'are', 0.99999994), (0, 'extremely', 1.0), (0, 'adaptable', 1.0)]]\n",
      "369 [[(0, 'cut', 1.0), (0, 'living', 1.0), (0, 'costs', 1.0)]]\n",
      "370 []\n",
      "371 [[(0, 'also', 1.0), (0, 'enjoys', 1.0), (0, 'group', 1.0), (0, 'interaction', 0.99999994)]]\n",
      "372 [[(0, 'notice', 1.0), (0, 'small', 0.99999994), (0, 'changes', 1.0)]]\n",
      "373 [[(0, 'birthday', 1.0), (0, 'claps', 1.0)]]\n",
      "374 [[(0, 'find', 1.0), (0, 'newly', 1.0), (0, 'laid', 1.0), (0, 'turtle', 1.0), (0, 'eggs', 1.0)]]\n",
      "375 [[(0, 'sand', 1.0), (0, 'piles', 0.99999994), (0, 'up', 1.0), (0, 'gradually', 1.0)]]\n",
      "376 [[(0, 'black', 1.0), (0, 'food', 1.0)]]\n",
      "377 []\n",
      "378 [[(0, 'loved', 0.99999994), (0, 'handmade', 1.0), (0, 'things', 1.0)]]\n",
      "379 [[(0, 'teenagers', 0.99999994)]]\n",
      "380 [[(0, 'be', 1.0), (0, 'better', 0.99999994), (0, 'readers', 1.0)]]\n",
      "381 [[(0, 'environmental', 1.0), (0, 'problems', 1.0)]]\n",
      "382 [[(0, 'once', 1.0), (0, 'cloned', 1.0000001), (0, 'goats', 0.99999994), (0, 'successfully', 1.0)]]\n",
      "383 [[(0, 'carry', 0.99999994), (0, 'water', 1.0)]]\n",
      "384 [[(0, 'was', 1.0), (0, 'handsome', 1.0)]]\n",
      "385 [[(0, 'drink', 1.0), (0, 'enough', 1.0), (0, 'water', 1.0)]]\n",
      "386 [[(0, 'help', 1.0), (0, 'people', 1.0), (0, 'develop', 1.0), (0, 'environmentally', 1.0), (0, 'friendly', 1.0), (0, 'habits', 1.0)]]\n",
      "387 [[(0, 'provide', 1.0), (0, 'gift', 1.0), (0, 'baskets', 1.0)]]\n",
      "388 [[(0, 'survive', 1.0), (0, 'bear', 1.0), (0, 'encounters', 1.0)]]\n",
      "389 [[(0, 'pronunciation', 1.0)]]\n",
      "390 [[(0, 'secondary', 1.0), (0, 'school', 1.0)]]\n",
      "391 []\n",
      "392 [[(0, 'did', 1.0), (0, 'not', 1.0), (0, 'own', 1.0), (0, 'automobiles', 1.0)]]\n",
      "393 [[(0, 'cook', 1.0), (0, 'food', 1.0)]]\n"
     ]
    }
   ],
   "source": [
    "import_data(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
